{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: Pulling without specifying how to reconcile divergent branches is\n",
      "hint: discouraged. You can squelch this message by running one of the following\n",
      "hint: commands sometime before your next pull:\n",
      "hint: \n",
      "hint:   git config pull.rebase false  # merge (the default strategy)\n",
      "hint:   git config pull.rebase true   # rebase\n",
      "hint:   git config pull.ff only       # fast-forward only\n",
      "hint: \n",
      "hint: You can replace \"git config\" with \"git config --global\" to set a default\n",
      "hint: preference for all repositories. You can also pass --rebase, --no-rebase,\n",
      "hint: or --ff-only on the command line to override the configured default per\n",
      "hint: invocation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "Welcome to JupyROOT 6.22/06\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "## IMPORTANT: Make sure that \"Uniandes_Framework\" is in .gitignore\n",
    "framework_path = \"Uniandes_Framework\"\n",
    "\n",
    "if os.path.exists(framework_path):\n",
    "    # Pull updates if the framework is already cloned\n",
    "    try:\n",
    "        subprocess.run([\"git\", \"-C\", framework_path, \"pull\"])\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise Exception(f\"Error occurred while pulling updates from the framework: {e}\")\n",
    "else:\n",
    "    # Clone the framework if it is not already cloned\n",
    "    try:\n",
    "        subprocess.run([\"git\", \"clone\", \"git@github.com:Phenomenology-group-uniandes/Uniandes_Framework.git\"])\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise Exception(f\"Error occurred while cloning the framework: {e}\")\n",
    "from Uniandes_Framework.delphes_reader.lhereader import LHE_Loader, readLHEF, get_event_by_child\n",
    "from Uniandes_Framework.delphes_reader.root_analysis import get_kinematics_row, make_histograms, overlap_histos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = \"woRHC_UpZp\"\n",
    "sim_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))), \"SIMULATIONS\", f\"lq_zp_{case}\")\n",
    "signals = [\"ta_ta\",\"zp_tau_tau\"]\n",
    "xs_path = { signal: os.path.join( os.path.dirname(os.getcwd()), \"01_signal_production\", f\"xs_signals_{case}\", signal, \"XS_Matrix.csv\") for signal in signals}\n",
    "# xs_path = { signal: 1 for signal in signals}\n",
    "M_U= [1000]\n",
    "G_U = [1.5]\n",
    "\n",
    "csv_sim_path = pd.read_csv(os.path.join(\"Uniandes_Framework\", \"SimulationsPaths.csv\"))\n",
    "df = pd.concat(\n",
    "    [csv_sim_path]\n",
    "    +\n",
    "    [ pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"name\" : signal + f\"_m{m}_g{g}\", \n",
    "        \"path\": os.path.join(sim_path, signal,f'M{m}_gU{g:.4f}'.replace('.', '_')),  \n",
    "        'xs(pb)': 1.0\n",
    "    }, \n",
    "    orient = \"index\").T for signal, m , g in product(signals,M_U,G_U) ]\n",
    ")\n",
    "# save the new csv\n",
    "csv_sim_path = os.path.join(os.getcwd(), \"SimulationsPaths.csv\")\n",
    "df.to_csv(csv_sim_path, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kinematics(run):\n",
    "    run_name = os.path.basename(os.path.dirname(run))\n",
    "    childs=readLHEF(run)\n",
    "    if os.path.isfile(run):\n",
    "        print('Reading', run_name)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    results = list()\n",
    "    for child in childs:\n",
    "        event = get_event_by_child(child)\n",
    "        taus = event.getParticlesByIDs([-15,15])\n",
    "        if len(taus) == 2:\n",
    "            taus[0].SetName(\"#tau_{1}\")\n",
    "            taus[1].SetName(\"#tau_{2}\")\n",
    "            sum_taus = taus[0].TLV + taus[1].TLV\n",
    "        else :\n",
    "            continue\n",
    "        row = get_kinematics_row(taus)\n",
    "        row[\"m_ll\"] = sum_taus.M()\n",
    "        results.append(row)\n",
    "    print(run_name, 'done!')\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the simulation\n",
    "\n",
    "def load_simulation(x):\n",
    "    signal, m, g = x\n",
    "    lhe_loader = LHE_Loader(signal + f\"_m{m}_g{g}\", csv_sim_path)\n",
    "    results = list()\n",
    "    \n",
    "    n_cores = 3\n",
    "    with mp.Pool(n_cores) as pool:\n",
    "        for result in list(pool.map(get_kinematics, lhe_loader.Forest)):\n",
    "            results += result\n",
    "    return signal, pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ta_ta_m1000_g1.5 imported with 15 trees!\n",
      "/home/Cristian/SIMULATIONS/lq_zp_woRHC_UpZp/ta_ta/M1000_gU1_5000\n",
      "Reading run_01\n",
      "Reading run_09\n",
      "Reading run_05\n",
      "run_01 done!\n",
      "run_09 done!\n",
      "run_05 done!\n",
      "Reading run_02\n",
      "Reading run_06\n",
      "Reading run_10\n",
      "run_02 done!\n",
      "run_06 done!\n",
      "run_10 done!\n",
      "Reading run_03\n",
      "Reading run_07\n",
      "Reading run_11\n",
      "run_03 done!\n",
      "run_07 done!\n",
      "run_11 done!\n",
      "Reading run_04\n",
      "Reading run_08\n",
      "Reading run_12\n",
      "run_04 done!\n",
      "run_08 done!\n",
      "run_12 done!\n",
      "Reading run_13\n",
      "run_13 done!\n",
      "Reading run_14\n",
      "run_14 done!\n",
      "Reading run_15\n",
      "run_15 done!\n"
     ]
    },
    {
     "ename": "ParseError",
     "evalue": "mismatched tag: line 575743, column 2 (<string>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31mRemoteTraceback\u001b[0m\u001b[0;31m:\u001b[0m \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib64/python3.9/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/tmp/ipykernel_116259/702447712.py\", line 3, in get_kinematics\n    childs=readLHEF(run)\n  File \"/home/Cristian/github/lq_zprime/02_parton_analysis/Uniandes_Framework/delphes_reader/lhereader.py\", line 183, in readLHEF\n    tree = ET.parse(lhe_file)\n  File \"/usr/lib64/python3.9/xml/etree/ElementTree.py\", line 1229, in parse\n    tree.parse(source, parser)\n  File \"/usr/lib64/python3.9/xml/etree/ElementTree.py\", line 580, in parse\n    self._root = parser._parse_whole(source)\nxml.etree.ElementTree.ParseError: mismatched tag: line 575743, column 2\n\"\"\"\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/usr/local/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3508\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[6], line 1\u001b[0m\n    a = dict(map(load_simulation, product(signals,M_U,G_U)))\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[5], line 10\u001b[0m in \u001b[1;35mload_simulation\u001b[0m\n    for result in list(pool.map(get_kinematics, lhe_loader.Forest)):\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/usr/lib64/python3.9/multiprocessing/pool.py:364\u001b[0m in \u001b[1;35mmap\u001b[0m\n    return self._map_async(func, iterable, mapstar, chunksize).get()\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/usr/lib64/python3.9/multiprocessing/pool.py:771\u001b[0;36m in \u001b[0;35mget\u001b[0;36m\n\u001b[0;31m    raise self._value\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>\u001b[0;36m\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m mismatched tag: line 575743, column 2\n"
     ]
    }
   ],
   "source": [
    "a = dict(map(load_simulation, product(signals,M_U,G_U)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a['ta_ta'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_hist_bins_dict={\n",
    "    \"#Delta{R}\":[96,0,7],\n",
    "    \"#Delta{#eta}\":[80,-5,5],\n",
    "    \"#Delta{#phi}\":[52,-3.25,3.25],\n",
    "    \"#Delta{pT}\":[120, 0.0, 1500.0],\n",
    "    \"#Delta{#vec{pT}}\":[240, 0.0, 4800.0],\n",
    "    \"#Delta{#vec{p}}\":[240, 0.0, 4800.0],\n",
    "    \"MET(GeV)\":[80, 0.0, 1000.0],\n",
    "    \"pT_\": [160, 0.0, 2000.0],\n",
    "    \"sT(GeV)\": [200, 0.0, 4000.0],\n",
    "    \"mT(GeV)\": [200, 0.0, 4000.0],\n",
    "    \"#eta_\":[80, -5, 5],\n",
    "    \"#phi_\":[128, -3.2, 3.2],\n",
    "    \"Energy_\":[80, 0.0, 1000.0],\n",
    "    \"m_ll\" : [160, 0.0, 2000.0]\n",
    "}\n",
    "root_histos ={ signal: make_histograms(a[signal],hist_bins_dict = default_hist_bins_dict) for signal in signals}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_histos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "histos, canvas, legend = overlap_histos( 'pT_{#tau_{1}}(GeV)', root_histos, log_scale =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "histos, canvas, legend = overlap_histos( 'pT_{#tau_{2}}(GeV)', root_histos, log_scale =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "histos, canvas, legend = overlap_histos('#Delta{R}_{#tau_{1}#tau_{2}}', root_histos, log_scale =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "histos, canvas, legend = overlap_histos('m_ll', root_histos, log_scale =True)\n",
    "canvas.SaveAs(\"mll.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
