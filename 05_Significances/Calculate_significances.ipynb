{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood ratio test\n",
    "To Calculate the significance of the difference between two nested models, we use the likelihood ratio test. The likelihood ratio test is based on the likelihood ratio, which expresses how many times more likely the data are under one model than the other. The likelihood ratio is defined as:\n",
    "$$\n",
    "\\Lambda = \\frac{\\mathcal{L}(\\theta_1)}{\\mathcal{L}(\\theta_0)}\n",
    "$$\n",
    "where $\\mathcal{L}(\\theta_1)$ is the likelihood of the data under the alternative hypothesis, and $\\mathcal{L}(\\theta_0)$ is the likelihood of the data under the null hypothesis. For us, the null hypothesis is the standard model, the background. And the alternative hypothesis our extension to the standard model, new particicles in the extension are the leptoquarks and $Z'$ bosons that contributes with extra events known as the signal, *i.e* the null hipotesis is the SM background and the alternative hipotesis is the SM background plus the signal. The likelihood ratio test is defined as:\n",
    "$$\n",
    "\\lambda = -2\\ln\\Lambda = -2\\ln\\frac{\\mathcal{L}(\\theta_1)}{\\mathcal{L}(\\theta_0)}\n",
    "$$\n",
    "## Likelihood ratio for binned counts\n",
    "In each bin the likelihood is defined as a Poisson distribution, then for a histogram with $N_{bins}$ bins, the likelihood is defined as:\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\prod_{i=1}^{N_{bins}} \\frac{\\mu_i^{n_i}e^{-\\mu_i}}{n_i!}\n",
    "$$\n",
    "where $n_i$ is the number of events in the bin $i$, and $\\mu_i$ is the expected number of events in the bin $i$ under the hypothesis $\\theta$.\n",
    "\n",
    "For our case, the null hypothesis is the background, and the alternative hypothesis is the background plus the signal. Then, the likelihood ratio in each bin is gived by\n",
    "$$\n",
    "\\Lambda_i = \\frac{\\mathcal{L}_i(\\theta_1)}{\\mathcal{L}_i(\\theta_0)} =\\frac{\\frac{e^{-\\left(s_i+b_i\\right)}\\left(s_i+b_i\\right)^{n_i^{\\text {cand }}}}{n_i^{\\text {cand } !}}}{e^{-b_i} \\frac{b_i^{n_i^{\\text {cand }}}}{n_i^{\\text {cand }} !}}\n",
    "$$ \n",
    "or in a useful form\n",
    "$$\n",
    "\\lambda_i=-2 \\ln \\Lambda_i=2 s_i-2 n_i \\ln \\left(1+\\frac{s_i}{b_i}\\right)\n",
    "$$\n",
    "where $s_i$ is the number of signal events in the bin $i$, and $b_i$ is the number of background events in the bin $i$.\n",
    "\n",
    "The Likelihood ratio for the whole histogram is the product of the likelihood ratio in each bin, then\n",
    "$$\n",
    "\\Lambda = \\prod_{i=1}^{N_{bins}} \\Lambda_i \\;\\; \\Longrightarrow  \\;\\; \\lambda = -2\\ln\\Lambda = -2\\sum_{i=1}^{N_{bins}} \\ln \\Lambda_i = \\sum_{i=1}^{N_{bins}} \\lambda_i\n",
    "$$\n",
    "Let me define some useful quantities:\n",
    "- LR weight:\n",
    "$$\n",
    "w_i = \\ln \\left( 1 + \\frac{s_i}{b_i} \\right)\n",
    "$$\n",
    "- Mean of the $\\lambda_i$ for \"b\" expts:\n",
    "$$\n",
    "\\left< \\lambda_i \\right>_b = 2s_i -2 b_i w_i\n",
    "$$\n",
    "- Mean of the $\\lambda_i$ for \"s+b\" expts:\n",
    "$$\n",
    "\\left< \\lambda_i \\right>_{s+b} = 2s_i -2 (s_i+b_i) w_i\n",
    "$$\n",
    "- RMS for poisson distribution:\n",
    "$$\n",
    "\\sigma_{n_i}^2 = n_i\n",
    "$$\n",
    "- RMS for $\\lambda_i$ for \"s+b\":\n",
    "$$\n",
    "\\sigma_{s_i+b_i}^2 = 4(s_i+b_i)w_i^2\n",
    "$$\n",
    "- RMS for $\\lambda_i$ for \"b\":\n",
    "$$\n",
    "\\sigma_{b_i}^2 = 4b_iw_i^2\n",
    "$$\n",
    "Significance is defined as the number of standard deviations that the null hypothesis is away from the alternative hypothesis, then\n",
    "$$\n",
    "\\kappa=\\frac{\\langle \\lambda\\rangle_b-N \\sigma_b-\\langle\\lambda\\rangle_{s+b}}{\\sigma_{s+b}}\n",
    "$$\n",
    "where $N$ is the expected number of background sigmas in the critical region.\n",
    "\n",
    "Simplyfing the expression for the optimized power of analysis:\n",
    "$$\n",
    "\\kappa=\\frac{\\sum s_i w_i-N \\sqrt{\\sum b_i w_i^2}}{\\sqrt{\\sum\\left(s_i+b_i\\right) w_i^2}}\n",
    "$$\n",
    "\n",
    "For a single bin, and strong signal, the optimized power of analysis is:\n",
    "$$\n",
    "\\kappa=\\frac{s}{\\sqrt{s+b}}\n",
    "$$\n",
    "\n",
    "With numpy, we can calculate the optimized power for the full histogram with the following code:\n",
    "  \n",
    "```python\n",
    "  def approx_global_sig(sig, bkg, N = 0.0) :\n",
    "    w = np.log(1. + sig/(bkg + 1e-9))\n",
    "    s_w = sig * w\n",
    "    b_w = bkg * w\n",
    "    s_ww = sig * w ** 2\n",
    "    b_ww = bkg * w ** 2\n",
    "    num = np.sum(s_w) - N * np.sqrt(np.sum(b_ww))\n",
    "    den = np.sqrt(np.sum(s_ww + b_ww))\n",
    "    return num / den\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def approx_global_sig(sig: np.array, bkg: np.array, N: float = 0.0) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the statistical significance of a signal over background in a given\n",
    "    dataset using a modified version of the formula (S -N sqrt(B))/sqrt(S+B), \n",
    "    where S is the number of signal events, B is the number of background events,\n",
    "    and N is the expected number of background sigmas in the signal region. \n",
    "\n",
    "    Parameters:\n",
    "    sig (np.array): Array containing the number of signal events in each bin.\n",
    "    bkg (np.array): Array containing the number of background events in each bin.\n",
    "    N (float): Expected number of background events in the signal region. \n",
    "        Default value is 0.0.\n",
    "\n",
    "    Returns:\n",
    "    float: The statistical significance of the signal.\n",
    "    \"\"\"\n",
    "    # Calculate weight factor w for each bin\n",
    "    w = np.log(1. + sig / (bkg + 1e-9))\n",
    "\n",
    "    # Calculate intermediate quantities\n",
    "    s_w = sig * w\n",
    "    b_w = bkg * w\n",
    "    s_ww = sig * w ** 2\n",
    "    b_ww = bkg * w ** 2\n",
    "\n",
    "    # Calculate numerator and denominator of modified formula\n",
    "    num = np.sum(s_w) - N * np.sqrt(np.sum(b_ww))\n",
    "    den = np.sqrt(np.sum(s_ww + b_ww))\n",
    "\n",
    "    # Calculate statistical significance and return it\n",
    "    return num / den"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of events for signal and backgrounds are calculated in the following way:\n",
    "$$\n",
    "N_{S} = \\sigma_{S} \\times \\mathcal{L} \\times \\epsilon_{S} \\\\\n",
    "$$\n",
    "where $\\sigma_{S}$ is the cross section of the signal process, $\\mathcal{L}$ is the integrated luminosity and $\\epsilon_{S}$ is the efficiency of the signal process. The efficiency is calculated by dividing the number of events that pass the selection criteria by the total number of events. The number of events for the background processes are calculated in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cases = [\"woRHC\"]\n",
    "\n",
    "limits = ['zp_upper_limit', 'zp_lower_limit']\n",
    "\n",
    "bkgs =  ['tbart', 'V+jets', 'stop', 'Diboson']\n",
    "\n",
    "channels = {\n",
    "    'hadronic_dLQ': 'hadronic_Tau_Tau_b_b',\n",
    "    'hadronic_sLQ': 'hadronic_Tau_Tau_b',\n",
    "    'hadronic_non-resonant': 'hadronic_Tau_Tau',\n",
    "    'semileptonic_dLQ': 'semileptonic_Tau_Tau_b_b',\n",
    "    'semileptonic_sLQ': 'semileptonic_Tau_Tau_b',\n",
    "    'semileptonic_non-resonant': 'semileptonic_Tau_Tau'\n",
    "    }\n",
    "\n",
    "signals = [\n",
    "    \"total_tau_tau\",\n",
    "    ]\n",
    "\n",
    "M_U= [1000, 1500, 2000, 2500,3000,3500]\n",
    "G_U = [1.75]\n",
    "G_ZP = [0.0,0.5, 1.5, 2.5, 3.5]\n",
    "\n",
    "xs_folder = os.path.join(\n",
    "    os.path.dirname(os.getcwd()), \n",
    "    \"01_signal_production\", \n",
    "    \"xs_13TeV\"\n",
    "    )\n",
    "eff_folder = os.path.join(\n",
    "    os.path.dirname(os.getcwd()),\n",
    "     \"03_delphes_preselection\", \"Efficiencies\")\n",
    "significance_folder = os.path.join(os.getcwd(), \"Significances\")\n",
    "\n",
    "L = 137 * 1e3 # Integrated luminosity in pb^-1\n",
    "\n",
    "for case, signal, limit,gu in product(cases, signals,limits,G_U):\n",
    "    sigificance_path = os.path.join(\n",
    "        significance_folder,\n",
    "        case,\n",
    "        limit,\n",
    "        f\"significances_G_U_{gu}.csv\"\n",
    "        )\n",
    "    os.makedirs(os.path.dirname(sigificance_path), exist_ok=True)\n",
    "    xs_path = os.path.join(\n",
    "        xs_folder,\n",
    "        case,\n",
    "        limit,\n",
    "        signal,\n",
    "        f\"XS_matrix_{gu}.csv\"\n",
    "        )\n",
    "    xs_matrix = pd.read_csv(xs_path, index_col=0)\n",
    "    Significance_matrix = pd.DataFrame(index = G_ZP, columns = M_U)\n",
    "\n",
    "    for m, gzp in product(M_U,G_ZP):\n",
    "       \n",
    "        Matrix_Signal = np.zeros([11,6])\n",
    "        Matrix_BKG = np.zeros([11,6])\n",
    "        # N = L * xs * eff\n",
    "        xs = xs_matrix[str(m)][gzp]\n",
    "        for n, channel in enumerate(channels):\n",
    "\n",
    "            if m < 2500:\n",
    "                txt_folder = os.path.join(\n",
    "                    os.getcwd(), \"Data_LQS2023\", \n",
    "                    f\"Histograms_{case}\",f\"M{m}\",\n",
    "                    channels[channel]\n",
    "                    )\n",
    "                scale =1 \n",
    "            else:\n",
    "                txt_folder = os.path.join(\n",
    "                    os.getcwd(), \"Data_LQS2023\", \n",
    "                    f\"Histograms_{case}\",f\"M{2500}\",\n",
    "                    channels[channel]\n",
    "                )\n",
    "                scale = xs / xs_matrix[\"2500\"][gzp]\n",
    "\n",
    "            \n",
    "            high_per_bin = np.loadtxt(\n",
    "                os.path.join(txt_folder, f\"high_per_bin_tau_tau.txt\")\n",
    "                )\n",
    "            high_per_bin = high_per_bin*scale\n",
    "            if gzp != 0.0: \n",
    "                eff_path = os.path.join(\n",
    "                    eff_folder,\n",
    "                    case,\n",
    "                    limit,\n",
    "                    signal,\n",
    "                    channel,\n",
    "                    f\"eff_matrix_{gu}.csv\"\n",
    "                    )\n",
    "                eff_matrix = pd.read_csv(eff_path, index_col=0)\n",
    "\n",
    "               \n",
    "                eff = eff_matrix[str(m)][gzp]\n",
    "                N = L * xs * eff\n",
    "                high_per_bin = high_per_bin*N/sum(high_per_bin)\n",
    "\n",
    "            \n",
    "            Matrix_Signal[:, n] += high_per_bin\n",
    "\n",
    "            for bkg in bkgs:\n",
    "                Matrix_BKG[:, n] += np.loadtxt(\n",
    "                    os.path.join(txt_folder, f\"high_per_bin_{bkg}.txt\")\n",
    "                    )\n",
    "        Signal_Data = np.asarray(Matrix_Signal.reshape((1,66)))\n",
    "        BKG_Data = np.asarray(Matrix_BKG.reshape((1,66)))\n",
    "        Significance_matrix[m][gzp] = approx_global_sig(Signal_Data, BKG_Data)\n",
    "    \n",
    "    Significance_matrix.to_csv(sigificance_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/disco4/personal_folders/c.rodriguez45/github/lq_zprime/05_Significances'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
