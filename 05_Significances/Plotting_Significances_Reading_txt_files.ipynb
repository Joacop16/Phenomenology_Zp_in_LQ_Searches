{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e2593-90bf-4ee1-ad08-673ecbf73972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "import os\n",
    "\n",
    "import subprocess\n",
    "\n",
    "## IMPORTANT: Make sure that \"Uniandes_Framework\" is in .gitignore\n",
    "framework_path = \"Uniandes_Framework\"\n",
    "\n",
    "if os.path.exists(framework_path):\n",
    "    # Pull updates if the framework is already cloned\n",
    "    try:\n",
    "        subprocess.run([\"git\", \"-C\", framework_path, \"pull\"])\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise Exception(f\"Error occurred while pulling updates from the framework: {e}\")\n",
    "else:\n",
    "    # Clone the framework if it is not already cloned\n",
    "    try:\n",
    "        subprocess.run([\"git\", \"clone\", \"git@github.com:Phenomenology-group-uniandes/Uniandes_Framework.git\"])\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise Exception(f\"Error occurred while cloning the framework: {e}\")\n",
    "        \n",
    "from Uniandes_Framework.heatmaps_utilities.functions import smooth, plot_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b9991-3593-49fb-8e0b-246f25633b45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def approx_global_sig(sig: np.array, bkg: np.array, N: float = 0.0) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the statistical significance of a signal over background in a given dataset using a modified version of\n",
    "    the formula (S -N sqrt(B))/sqrt(S+B), where S is the number of signal events, B is the number of background events,\n",
    "    and N is the expected number of background events in the signal region.\n",
    "\n",
    "    Parameters:\n",
    "    sig (np.array): 1D array containing the number of signal events in each bin of the dataset.\n",
    "    bkg (np.array): 1D array containing the number of background events in each bin of the dataset.\n",
    "    N (float): Expected number of background events in the signal region. Default value is 0.0.\n",
    "\n",
    "    Returns:\n",
    "    float: The statistical significance of the signal over background in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate weight factor w for each bin\n",
    "    w = np.log(1. + sig/(bkg + 1e-9))\n",
    "\n",
    "    # calculate intermediate quantities\n",
    "    s_w = sig * w\n",
    "    b_w = bkg * w\n",
    "    s_ww = sig * w ** 2\n",
    "    b_ww = bkg * w ** 2\n",
    "\n",
    "    # calculate numerator and denominator of modified formula\n",
    "    num = np.sum(s_w) - N * np.sqrt(np.sum(b_ww))\n",
    "    den = np.sqrt(np.sum(s_ww + b_ww))\n",
    "\n",
    "    # calculate statistical significance and return it\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd2935-45c6-4557-aee2-aaf4e968c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = 'tau_tau'\n",
    "\n",
    "bkgs =  ['tbart', 'V+jets', 'stop', 'Diboson']\n",
    "Masses = [\"1000\", '1250', '1500', '1750', '2000', '2250', '2500']\n",
    "betards = ['wRHC', 'woRHC']\n",
    "suffix_by_betard = {'wRHC': '', 'woRHC': '_wo_RHC'}\n",
    "\n",
    "channels = {'hadronic_dLQ': 'hadronic_Tau_Tau_b_b',\n",
    "           'hadronic_sLQ': 'hadronic_Tau_Tau_b',\n",
    "           'hadronic_non-resonant': 'hadronic_Tau_Tau',\n",
    "           'semileptonic_dLQ': 'semileptonic_Tau_Tau_b_b',\n",
    "           'semileptonic_sLQ': 'semileptonic_Tau_Tau_b',\n",
    "           'semileptonic_non-resonant': 'semileptonic_Tau_Tau'}\n",
    "\n",
    "XS_Dict = {}\n",
    "\n",
    "for signal in ['ta_ta', 'zp_tau_tau']:\n",
    "    Path = os.path.join(os.path.dirname(os.getcwd()), \"01_signal_production\", 'xs_signals_woRHC', signal, 'XS_Matrix.csv')\n",
    "    \n",
    "    XS_Dict[signal] = pd.read_csv(Path, index_col= 0)\n",
    "    XS_Dict[signal].columns = [float(i) for i in XS_Dict[signal].columns]\n",
    "    XS_Dict[signal].index = [float(i) for i in XS_Dict[signal].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445b819-e6af-4e7a-a791-7b1e97eb1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "XS_Dict[signal].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6e106-dede-4d22-b08e-1c12a85fbbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gU = 1.5\n",
    "gU_name = str(gU).replace('.','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb25869-c21e-42df-83c4-7bacc2886af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Significances = {'wRHC': {}, 'woRHC': {}}\n",
    "\n",
    "#non-res:\n",
    "for betard in betards:\n",
    "    Significances[betard]['non-res'] = {}\n",
    "    \n",
    "    for Mass in Masses:\n",
    "        Significances[betard]['non-res'][float(Mass)] = {} \n",
    "\n",
    "        for coupling in XS_Dict['zp_tau_tau'].index:\n",
    "            \n",
    "            Matrix_Signal = np.zeros([11,6])\n",
    "            Matrix_BKG = np.zeros([11,6])\n",
    "            for n, channel in enumerate(channels):\n",
    "                \n",
    "                Efficiences = pd.read_csv(os.path.join(f'Data_LQS2023/Efficiences', f'{channel}.csv'), index_col= 0)\n",
    "                Efficience = Efficiences[f'Tau_Tau{suffix_by_betard[betard]}_{Mass}']['DeltaR > 0.3']\n",
    "                XS_Total = XS_Dict['ta_ta'][float(Mass)][gU] + XS_Dict['zp_tau_tau'][float(Mass)][coupling]\n",
    "                Luminosity = 137*1000\n",
    "                N_events = Efficience*XS_Total*Luminosity\n",
    "                \n",
    "                path_to_txt = os.path.join(f'Data_LQS2023/Histograms_{betard}', f'M{Mass}', channels[channel], f\"high_per_bin_tau_tau.txt\")\n",
    "                high_per_bin = np.loadtxt(path_to_txt)\n",
    "                high_per_bin = high_per_bin/sum(high_per_bin)                \n",
    "\n",
    "                Matrix_Signal[:, n] += high_per_bin*N_events\n",
    "\n",
    "                for bkg in bkgs:\n",
    "                    path_to_txt = os.path.join(f'Data_LQS2023/Histograms_{betard}', f'M{Mass}', channels[channel], f\"high_per_bin_{bkg}.txt\")\n",
    "                    Matrix_BKG[:, n] += np.loadtxt(path_to_txt)                \n",
    "\n",
    "            Signal_Data = np.asarray(Matrix_Signal.reshape((1,66)))\n",
    "            BKG_Data = np.asarray(Matrix_BKG.reshape((1,66)))    \n",
    "            Significances[betard]['non-res'][float(Mass)][coupling/gU] = approx_global_sig(Signal_Data, BKG_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50c7ef-485b-4605-86a9-3853cea57912",
   "metadata": {},
   "outputs": [],
   "source": [
    "for betard in betards:\n",
    "    file_name = f'Significances/Significance_Table_13TeV_L137_non-res_{betard}_with_gU_{gU_name}.csv'\n",
    "    pd.DataFrame.from_dict(Significances[betard]['non-res']).to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1745cd7-22e3-4e64-8ea0-f690a4ddb13b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "betard_title = {'wRHC': r'$, \\mathbf{\\beta_{R} = -1}$', 'woRHC': r'$, \\mathbf{\\beta_{R} = 0}$'}\n",
    "\n",
    "# ubicaciones = {'non-res_wRHC': [[1600/1000,1], [1600/1000, 1.2], [1600/1000, 1.5]],\n",
    "#                'non-res_woRHC': [[1600/1000,0.8], [1600/1000, 1.25], [1600/1000, 1.5]], \n",
    "#               }\n",
    "\n",
    "def Calcular_g_U(c_U, M):\n",
    "    v = 246\n",
    "    return 2*np.sqrt(c_U)*M/v\n",
    "\n",
    "for betard in betards:\n",
    "    \n",
    "    Data = pd.DataFrame.from_dict(Significances[betard]['non-res'])\n",
    "    Data.columns = [float(column)/1000 for column in Data.columns] #TeV\n",
    "\n",
    "    Data_interpolate = smooth(Data, log = True)\n",
    "    # Data_interpolate = np.log10(Data)\n",
    "\n",
    "    fig, ax, curves = plot_heatmap(Data_interpolate,\n",
    "                           level_curves = {np.log10(1.69) : r'$1.69 \\sigma$', np.log10(3): r'$3 \\sigma$', np.log10(5): r'$5 \\sigma$'},\n",
    "                           # curves_labels_locations= ubicaciones[f'non-res_{betard}'],\n",
    "                           title_right = r'$\\mathbf{\\sqrt{s} = 13 TeV , L = 137 fb^{-1}, g_{U} = ' + f'{gU}' + '}$' + betard_title[betard] ,\n",
    "                           x_label = r'$M_{U}$ [TeV]', \n",
    "                           y_label = r'$g_{Z^{\\prime}} / g_U$',\n",
    "                           cbar_label = r'$\\log_{10}$(Significance)',\n",
    "                          )        \n",
    "\n",
    "    file_name = f'Significances/Significance_Heatmap_13TeV_L137_non-res_{betard}_with_gU_{gU_name}.png'\n",
    "    plt.savefig(file_name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1adc4-3391-47da-b7f6-0007c01e21da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "sigma_curves = {}\n",
    "for betard in betards:\n",
    "    sigma_curves[betard] = {}\n",
    "    \n",
    "for betard in betards:\n",
    "    \n",
    "    Data = pd.DataFrame.from_dict(Significances[betard]['non-res'])\n",
    "    Data.columns = [float(column)/1000 for column in Data.columns] #TeV\n",
    "\n",
    "    Data_interpolate = smooth(Data, log = True)\n",
    "    fig, ax, sigma_curves[betard]['non-res'] = plot_heatmap(Data_interpolate, level_curves = {np.log10(1.69) : '' , np.log10(3): '', np.log10(5): ''})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7c9f9-039a-4d29-ab0a-d13ae7f4f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save sigma_curves in order to plot curves in other jupyter notebook:\n",
    "sigma_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f697a0d0-816d-4739-93e1-c703532f3ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# file = open(\"Significances/sigma_curves_137.pkl\",\"wb\")\n",
    "# pickle.dump(sigma_curves,file)\n",
    "# file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
